{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9414eece",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2593351",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lstm import LSTM\n",
    "from ddpm import DDPM, ContextUnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e5cd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sine_dataset(samples=100, seq_len=50):\n",
    "    data = []\n",
    "    for _ in range(samples):\n",
    "        # Random frequency and phase to make it robust\n",
    "        freq = np.random.uniform(0.5, 2.0)\n",
    "        phase = np.random.uniform(0, 2*np.pi)\n",
    "        t = np.linspace(0, 10, seq_len)\n",
    "        \n",
    "        # Signal = Sine + slight noise\n",
    "        signal = np.sin(freq * t + phase) + 0.05 * np.random.randn(seq_len)\n",
    "        data.append(signal)\n",
    "    \n",
    "    # Shape: [Batch, Seq_Len, Input_Size=1]\n",
    "    return torch.FloatTensor(np.array(data)).unsqueeze(2)\n",
    "\n",
    "def create_shapes_dataset(samples=100, size=32):\n",
    "    data = []\n",
    "    for _ in range(samples):\n",
    "        img = np.zeros((size, size))\n",
    "        \n",
    "        # Random Square\n",
    "        x = np.random.randint(5, size-10)\n",
    "        y = np.random.randint(5, size-10)\n",
    "        w = np.random.randint(5, 10)\n",
    "        img[x:x+w, y:y+w] = 1.0\n",
    "        \n",
    "        data.append(img)\n",
    "        \n",
    "    # Shape: [Batch, Channels=1, Height, Width]\n",
    "    return torch.FloatTensor(np.array(data)).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c1beed",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = create_sine_dataset(1000,50)\n",
    "samples.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79f9c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_shape = create_shapes_dataset(100,32)\n",
    "samples_shape.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124071d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(4):\n",
    "    rand_index = random.randint(0, 100)\n",
    "    plt.plot(samples[rand_index,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d21deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lstm(model, data, epochs=50):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    print(\"--- Training LSTM ---\")\n",
    "    model.train()\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #print(data[0, :, :].size())\n",
    "        predictions, _ = model(data)\n",
    "        \n",
    "        # Target: We want the model to predict the NEXT step.\n",
    "        # Input at t should predict Data at t+1.\n",
    "        # We crop the last prediction and the first data point to align them.\n",
    "        preds_shifted = predictions[:, :-1, :]\n",
    "        targets_shifted = data[:, 1:, :]\n",
    "        \n",
    "        loss = criterion(preds_shifted, targets_shifted)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}: Loss {loss.item():.5f}\")\n",
    "            \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5926b667",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequence(model, seed_data, future_steps=50):\n",
    "    \"\"\"\n",
    "    Uses the trained model to generate future steps based on a seed sequence.\n",
    "    \n",
    "    Args:\n",
    "        model: The trained LSTM model\n",
    "        seed_data: Tensor of shape [1, Seq_Len, Input_Size] (a single sequence)\n",
    "        future_steps: How many steps to generate\n",
    "        \n",
    "    Returns:\n",
    "        generated_seq: Tensor of shape [1, Future_Steps, Input_Size]\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Initialize internal state\n",
    "    h = torch.zeros(1, model.hidden_size).to(seed_data.device)\n",
    "    c = torch.zeros(1, model.hidden_size).to(seed_data.device)\n",
    "    \n",
    "    generated_values = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # 1. Warm up the internal state (h, c) using the seed data\n",
    "        seq_len = seed_data.size(1)\n",
    "        for t in range(seq_len):\n",
    "            x_t = seed_data[:, t, :] # Shape (1, Input_Size)\n",
    "            h, c, _ = model.cell(x_t, h, c)\n",
    "        \n",
    "        # The model is now primed. The last 'h' contains the memory of the seed.\n",
    "        # We make the first prediction.\n",
    "        current_input = model.predictor(h) \n",
    "        generated_values.append(current_input)\n",
    "        \n",
    "        # 2. Autoregressive Generation Loop\n",
    "        for _ in range(future_steps - 1):\n",
    "            # Feed the LAST PREDICTION as the NEXT INPUT\n",
    "            h, c, _ = model.cell(current_input, h, c)\n",
    "            pred = model.predictor(h)\n",
    "            \n",
    "            generated_values.append(pred)\n",
    "            current_input = pred # Update input for next step\n",
    "            \n",
    "    return torch.stack(generated_values, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6621d8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create Data\n",
    "lstm_data = create_sine_dataset(samples=1000, seq_len=50)\n",
    "\n",
    "# 2. Train\n",
    "lstm_model = LSTM(input_size=1, hidden_size=16)\n",
    "loss = train_lstm(lstm_model, lstm_data, epochs=200)\n",
    "\n",
    "# Use the first sample from our dataset as a \"seed\"\n",
    "seed_sample = lstm_data[0:1, :, :] # Shape [1, 50, 1]\n",
    "\n",
    "future_steps = 50\n",
    "generated = generate_sequence(lstm_model, seed_sample, future_steps=future_steps)\n",
    "\n",
    "plt.plot(loss)\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a68ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Seed shape: {seed_sample.shape}\")\n",
    "print(f\"Generated shape: {generated.shape}\")\n",
    "print(f\"Generated values: {generated[0, :, 0].numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc1d7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(4):\n",
    "    future_steps = 50\n",
    "    generated_ = generate_sequence(lstm_model, seed_sample, future_steps=future_steps)\n",
    "    plt.plot(generated_[0,:,0])\n",
    "plt.plot(seed_sample[0,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda21f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_out, history = lstm_model(lstm_data)\n",
    "for _ in range(4):\n",
    "    rand_index = random.randint(0,100)\n",
    "    plt.plot(lstm_out[rand_index,:,0].detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7e7cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "history[\"forget\"][0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57bd22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history[\"forget\"][0][0].detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c091b96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ddpm(ddpm_model, data, epochs=50):\n",
    "    optimizer = optim.Adam(ddpm_model.network.parameters(), lr=1e-3)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    print(\"\\n--- Training DDPM ---\")\n",
    "    ddpm_model.train()\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        avg_loss = 0\n",
    "        # Simple batch processing (treating whole dataset as one batch for simplicity here)\n",
    "        x0 = data \n",
    "        n = len(x0)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 1. Sample random timesteps for each image in batch\n",
    "        t = torch.randint(0, ddpm_model.n_steps, (n,))\n",
    "        \n",
    "        # 2. Generate random noise (The \"Inhibitor\" we want to predict)\n",
    "        epsilon = torch.randn_like(x0)\n",
    "        \n",
    "        # 3. Add noise to image (Forward Diffusion)\n",
    "        # Formula: x_t = sqrt(alpha_bar) * x0 + sqrt(1-alpha_bar) * epsilon\n",
    "        a_bar = ddpm_model.alpha_bars[t].view(-1, 1, 1, 1)\n",
    "        noisy_image = torch.sqrt(a_bar) * x0 + torch.sqrt(1 - a_bar) * epsilon\n",
    "        \n",
    "        # 4. Model attempts to predict the noise\n",
    "        noise_pred = ddpm_model.network(noisy_image, t)\n",
    "        \n",
    "        # 5. Loss: How close was the predicted noise to the actual noise?\n",
    "        loss = criterion(noise_pred, epsilon)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}: Loss {loss.item():.5f}\")\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1383e7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "def train_ddpm_on_mnist():\n",
    "    # --- Hyperparameters ---\n",
    "    n_epoch = 20 # Enough for MNIST digits to appear\n",
    "    batch_size = 128\n",
    "    n_T = 400 # Timesteps\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    lrate = 1e-4\n",
    "\n",
    "    # --- Data Loading ---\n",
    "    tf = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])\n",
    "    dataset = datasets.MNIST(\"./data\", train=True, download=True, transform=tf)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "    # --- Setup Schedules (The \"Inhibitor\" Physics) ---\n",
    "    beta_1 = 1e-4\n",
    "    beta_T = 0.02\n",
    "    betas = torch.linspace(beta_1, beta_T, n_T + 1).to(device)\n",
    "    alphas = 1 - betas\n",
    "    alphas_bar = torch.cumprod(alphas, dim=0)\n",
    "\n",
    "    # Pre-calculate standard DDPM constants to save compute\n",
    "    ddpm_schedules = {\n",
    "        \"sqrtab\": torch.sqrt(alphas_bar),\n",
    "        \"sqrtmab\": torch.sqrt(1 - alphas_bar),\n",
    "        \"oneover_sqrta\": 1 / torch.sqrt(alphas),\n",
    "        \"mab_over_sqrtmab\": (1 - alphas) / torch.sqrt(1 - alphas_bar),\n",
    "        \"sqrt_beta_t\": torch.sqrt(betas),\n",
    "    }\n",
    "\n",
    "    # --- Model Init ---\n",
    "    model = ContextUnet(in_channels=1, n_feat=64).to(device)\n",
    "    ddpm = DDPM(model, ddpm_schedules, n_T, device)\n",
    "    optim = torch.optim.Adam(ddpm.parameters(), lr=lrate)\n",
    "\n",
    "    # --- Training Loop ---\n",
    "    print(f\"Starts training on {device}...\")\n",
    "    \n",
    "    for ep in range(n_epoch):\n",
    "        ddpm.train()\n",
    "        pbar = torch.optim.lr_scheduler\n",
    "        loss_ema = None\n",
    "        \n",
    "        for x, _ in dataloader:\n",
    "            optim.zero_grad()\n",
    "            x = x.to(device)\n",
    "            \n",
    "            # DDPM Forward Pass\n",
    "            noise_pred, noise = ddpm(x)\n",
    "            \n",
    "            # Loss: Activator Error\n",
    "            loss = F.mse_loss(noise_pred, noise)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            \n",
    "            if loss_ema is None: loss_ema = loss.item()\n",
    "            else: loss_ema = 0.95 * loss_ema + 0.05 * loss.item()\n",
    "\n",
    "        print(f\"Epoch {ep:02d} | Loss: {loss_ema:.4f}\")\n",
    "        \n",
    "        # --- Visualization for sanity check ---\n",
    "        if ep % 5 == 0 or ep == n_epoch - 1:\n",
    "            ddpm.eval()\n",
    "            with torch.no_grad():\n",
    "                x_gen, _ = ddpm.sample(16, (1, 28, 28), device)\n",
    "                grid = make_grid(x_gen * -1 + 1, nrow=4) # Invert colors for visibility\n",
    "                plt.figure(figsize=(4,4))\n",
    "                plt.imshow(grid.permute(1, 2, 0).cpu().numpy())\n",
    "                plt.axis('off')\n",
    "                plt.title(f\"Generated at Epoch {ep}\")\n",
    "                plt.show()\n",
    "\n",
    "    return ddpm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c67a24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = train_ddpm_on_mnist()\n",
    "final_img, history = trained_model.sample(n_sample=1, size=(1, 28, 28), device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023959dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lstm-ddpm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
